/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Nextflow config file for running pipeline in AWS batch
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
*/

params {

    config_profile_name         = 'AWS batch execution'
    config_profile_description  = 'Parameters necessary for AWS batch execution'

    awsqueue                    = ''   //TODO: Set environment configuration base [System.getenv()]
    awsregion                   = ''   //TODO: Define region
    awscli                      = '/miniconda/bin/aws'

}

docker.registry                  = ''   //TODO: Set docker registry if you have a mirror of containers

aws.region                       = params.awsregion
aws.batch.cliPath                = params.awscli

// basic options
aws.client.endpoint              = 'https://s3.eu-central-1.amazonaws.com'
aws.client.maxConnections        = 12      // max no of allowed open HTTP connections
aws.client.maxErrorRetry         = 10      // max no of retries (for failed retryable requests)

aws.client.uploadMaxThreads      = 6       // max no of threads used for multipart upload
aws.client.uploadChunkSize       = '100MB' // size of a single part in a multipart upload (default 100 MB)
aws.client.uploadMaxAttempts     = 8       // max no of upload attempts after which a multipart upload returns an error (default: 5).
aws.client.uploadRetrySleep      = '2 sec' // time to wait after a failed upload attempt to retry the part upload (default: 500ms).

//aws.batch.maxParallelTransfers   = 2         // parallel upload/download transfer operations per job (default: 4).
//aws.batch.maxTransferAttempts    = 5         // max. no of downloads attempts from S3 (default: 1).
//aws.batch.delayBetweenAttempts   = 10        // delay between download attempts from S3 (default 10 sec).
    
process {
    executor      = 'awsbatch'
    queue         = params.awsqueue
}